<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Subhash Kantamneni</title>

  <!--------------------------------- Theme ----------------------------------->
  <link rel="stylesheet" href="stylesheets/styles.css">

  <!--------------------------------- Icons ----------------------------------->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.2.0/css/fork-awesome.min.css"
  integrity="sha256-XoaMnoYC5TH6/+ihMEnospgm0J1PM/nioxbOUdnM8HY=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

  <!--------------------------------- Other stuff ----------------------------->


  <style>
    body {
      background-color: white;
      color: #222222;
      font-size: 16.5px;
    }
    .social-icons {
      display: flex;
      justify-content: center;
      margin-top: 10px;
    }
    .social-icon i {
      color: #222222;
    }
    a {
      color: #0066cc;
    }
    h1 {
      font-size: 36px;
      text-align: center;
      font-weight: bold;
    }
    h2 {
      font-size: 24px;
      font-weight: bold;
    }
    .profile-img {
      display: block;
      margin: 0 auto;
      width: 70%;
      border-radius: 50%;
    }
  </style>

</head>

<body>
  <div class="wrapper">
    <header>
      <h1>Subhash Kantamneni</h1>
      <img src="assets/images/subhash.jpg" alt="Bio Image" class="profile-img">

      <div class="social-icons">
        <a class="social-icon" href="mailto:subhashk@mit.edu"><i
            class="fa fa-envelope-square"></i></a>
        <a class="social-icon" href="https://github.com/subhashk01"><i
            class="fa fa-github-square"></i></a>
        <a class="social-icon" href="https://scholar.google.com/citations?user=mF7NANoAAAAJ&hl=en&gmla=AOv-ny_NUY3N1YY6DJkfhySx36jXTeXIFGmfyAiM2G4gtsJlSyYF7-bD6hYodY72YWHlzxyisAf3KubhNZmzmsjTSXRYuuyWv1iAIGgR1vc&sciund=17608548661071717622"><i
            class="ai ai-google-scholar-square"></i></a>
        <a class="social-icon" href="assets/files/Subhash_CV.pdf"><i class="ai ai-cv-square"></i></a>
        <!-- Twitter icon -->
        <a class="social-icon" href="https://x.com/thesubhashk"><i class="fa fa-twitter-square"></i></a>
      </div>

      <br>

    </header>

    <section>
      <h1>What's Up</h1>

      Hey! I'm an Alignment Science researcher at Anthropic working on Sam Marks' Cognitive Oversight team. Previously, I was an EECS Master's student at MIT supervised by
      <a href="https://tegmark.org/">Max Tegmark</a>. 
      I'm primarily interested in AI Safety from two angles: 
      mechanistic interpretability, where I attempt to decode the underlying algorithms neural networks use, and alignment, where I explore 
      how to control and align AI systems with human values. 

      <br><br>     

      I received my undergraduate degree from MIT with a double major in Physics and
      Computer Science and a 5.0 GPA. During this time I won the Fulbright Scholarship, was accepted
      to MIT's Physics honor society in the top 10% of physics majors, advised MIT President Sally Kornbluth through the Presidential Advisory Cabinet, 
      and taught STEM workshops to high school students in South Korea, South Africa, Botswana, and Bahrain. 
      I love reading, meditating, traveling, and playing basketball (go Heat).
      <br>
      <br>

      <hr>
      <h2>Selected Papers</h2>


      <p>
        <b>Scaling Laws for Scalable Oversight</b> <br>
        Joshua Engels*, David Baek*, <u>Subhash Kantamneni</u>*, and Max Tegmark<br>
        <a href="https://arxiv.org/abs/2504.18530">Paper</a> |
        <a href="https://github.com/subhashk01/oversight-scaling-laws">Code</a> |
        <a href="https://x.com/thesubhashk/status/1917640398052241517">Twitter</a> | Submitted to NeurIPS 2025 Main Conference
      </p>
      

      <p>
        <b>Are Sparse Autoencoders Useful? A Case Study in Sparse Probing</b> <br>
        <u>Subhash Kantamneni</u>*, Joshua Engels*, Senthooran Rajamanoharan, Max Tegmark, and Neel Nanda<br>
        <i>Work done in Neel Nanda's MATS stream (3% acceptance rate)</i><br>
        <a href="https://arxiv.org/abs/2502.16681">Paper</a> |
        <a href="https://www.alignmentforum.org/posts/osNKnwiJWHxDYvQTD/takeaways-from-our-recent-work-on-sae-probing">Blog</a> |
        <a href="https://github.com/JoshEngels/SAE-Probes">Code</a> |
        <a href="https://x.com/thesubhashk/status/1894412347671761293">Twitter</a> | ICML 2025 Main Conference
      </p>

      <p>
        <b>Language Models Use Trigonometry to Do Addition</b> <br>
        <u>Subhash Kantamneni</u> and Max Tegmark<br>
        <a href="https://arxiv.org/abs/2502.00873">Paper</a> |
        <a href="https://www.lesswrong.com/posts/E7z89FKLsHk5DkmDL/language-models-use-trigonometry-to-do-addition-1">Blog</a> |
        <a href="https://github.com/subhashk01/LLM-addition">Code</a> |
        <a href="https://x.com/thesubhashk/status/1887138694546788556">Twitter</a> | ICLR 2025 Reasoning and Planning for LLMs Workshop
      </p>

      <p>
        <b>How Do Transformers Model Physics? Investigating the Simple Harmonic Oscillator</b> <br>
        <u>Subhash Kantamneni</u>, Ziming Liu, and Max Tegmark<br>
        <a href="https://arxiv.org/abs/2405.17209">Paper</a> |
        <a href="https://github.com/subhashk01/transformer-physics">Code</a> |
        <a href="https://x.com/thesubhashk/status/1795474076024320170">Twitter</a> | <i>Entropy</i> and ICML 2024 Mechanistic Interpretability Workshop
      </p>
      
      <p>
        <b>OptPDE: Discovering Novel Integrable Systems via AI-Human Collaboration</b> <br>
        <u>Subhash Kantamneni</u>, Ziming Liu, and Max Tegmark<br>
        <a href="https://arxiv.org/abs/2405.04484">Paper</a> |
        <a href="https://github.com/subhashk01/pde-sid">Code</a> |
        <a href="https://x.com/thesubhashk/status/1788643691231576162">Twitter</a> | <i>Physical Review E</i>
      </p>

      <p>
        <b>Enhancing Predictive Capabilities in Fusion Burning Plasmas Through Surrogate-Based Optimization in Core Transport Solvers</b> <br>
        P. Rodriguez-Fernandez, N.T. Howard, A. Saltzman, <u>Subhash Kantamneni</u>, J. Candy, C. Holland, M. Balandat, S. Ament, A.E. White<br>
        <a href="https://arxiv.org/abs/2312.12610">Paper</a> | <i>Nuclear Fusion</i>
      </p>


      <p>
        <b>NuCLR: Nuclear Co-Learned Representations</b> <br>
        Ouail Kitouni, Niklas Nolte, Sokratis Trifinopoulos, <u>Subhash Kantamneni</u>, and Mike Williams<br>
        <a href="https://arxiv.org/abs/2306.06099">Paper</a> | ICML 2023 SynS and ML Workshop
      </p>


      <hr>

      <!-- <h2>Other Projects and Writing</h2>

      <p>
        <a href="https://www.lesswrong.com/posts/NMLq8yoTecAF44KX9/sae-probing-what-is-it-good-for-absolutely-something">SAE Probing: What is it good for? Absolutely something!</a> (2024) - 
        We examine whether SAE probes are more data efficient and robust than activation probes.
      </p> -->

    </section>
    <footer>
      <p><small>Check out this website's source <a
            href="https://github.com/subhashk01/subhash-other-website">here</a>!</small><br>
        <small>Hosted on GitHub Pages with Jekyll. <br>
          Homepage theme by <a href="https://github.com/orderedlist">orderedlist</a>.</small>
      </p>
    </footer>
  </div>
  <script src="/javascripts/scale.fix.js"></script>
</body>

</html>
